{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2603715,"sourceType":"datasetVersion","datasetId":1582403},{"sourceId":7647312,"sourceType":"datasetVersion","datasetId":4457666}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/semtex6412/trees-ensemble-heart-failure-prediction-dataset?scriptVersionId=183926801\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\n#plt.style.use('./deeplearning.mplstyle')\n\nRANDOM_STATE = 55 ## ensures reproducibility","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-17T07:11:34.386301Z","iopub.execute_input":"2024-06-17T07:11:34.387397Z","iopub.status.idle":"2024-06-17T07:11:34.394639Z","shell.execute_reply.started":"2024-06-17T07:11:34.387349Z","shell.execute_reply":"2024-06-17T07:11:34.393106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Datatset\n- this dataset is obtained from: [Heart Failure Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)\n\n#### Context\n- Cardiovascular Disease (CVDs) is the number one cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of five CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs.\n- people with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management.  \n- this dataset contains 11 features that can be used to predict possible heart disease.\n- GOAL: train a machine learning model to assist with diagnosing this disease.\n\n#### Attribute Information\n- Age: age of the patient [years]\n- Sex: sex of the patient [M: Male, F: Female]\n- ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n- RestingBP: resting blood pressure [mm Hg]\n- Cholesterol: serum cholesterol [mm/dl]\n- FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n- RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n- MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n- ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n- Oldpeak: oldpeak = ST [Numeric value measured in depression]\n- ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n- HeartDisease: output class [1: heart disease, 0: Normal]","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/heart-failure-prediction/heart.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:13:03.894856Z","iopub.execute_input":"2024-06-17T07:13:03.895376Z","iopub.status.idle":"2024-06-17T07:13:03.918817Z","shell.execute_reply.started":"2024-06-17T07:13:03.895339Z","shell.execute_reply":"2024-06-17T07:13:03.91732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:13:20.1097Z","iopub.execute_input":"2024-06-17T07:13:20.110144Z","iopub.status.idle":"2024-06-17T07:13:20.142962Z","shell.execute_reply.started":"2024-06-17T07:13:20.110111Z","shell.execute_reply":"2024-06-17T07:13:20.14153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_variables = ['Sex',\n'ChestPainType',\n'RestingECG',\n'ExerciseAngina',\n'ST_Slope'\n]","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:13:30.333797Z","iopub.execute_input":"2024-06-17T07:13:30.33421Z","iopub.status.idle":"2024-06-17T07:13:30.340734Z","shell.execute_reply.started":"2024-06-17T07:13:30.334178Z","shell.execute_reply":"2024-06-17T07:13:30.339174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replaces the columns with the one-hot encoded ones and keep the columns outside 'columns' argument as it is.\ndf = pd.get_dummies(data = df,\n                         prefix = cat_variables,\n                         columns = cat_variables)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:14:13.454541Z","iopub.execute_input":"2024-06-17T07:14:13.455025Z","iopub.status.idle":"2024-06-17T07:14:13.481833Z","shell.execute_reply.started":"2024-06-17T07:14:13.45499Z","shell.execute_reply":"2024-06-17T07:14:13.48056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:14:23.677993Z","iopub.execute_input":"2024-06-17T07:14:23.678406Z","iopub.status.idle":"2024-06-17T07:14:23.709154Z","shell.execute_reply.started":"2024-06-17T07:14:23.678374Z","shell.execute_reply":"2024-06-17T07:14:23.707519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:22:27.643722Z","iopub.execute_input":"2024-06-17T07:22:27.644229Z","iopub.status.idle":"2024-06-17T07:22:27.680804Z","shell.execute_reply.started":"2024-06-17T07:22:27.644194Z","shell.execute_reply":"2024-06-17T07:22:27.678971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [x for x in df.columns if x not in 'HeartDisease'] ## ditch target variable","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:22:38.656197Z","iopub.execute_input":"2024-06-17T07:22:38.656742Z","iopub.status.idle":"2024-06-17T07:22:38.663557Z","shell.execute_reply.started":"2024-06-17T07:22:38.656696Z","shell.execute_reply":"2024-06-17T07:22:38.661923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(features))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:22:44.877632Z","iopub.execute_input":"2024-06-17T07:22:44.878101Z","iopub.status.idle":"2024-06-17T07:22:44.886078Z","shell.execute_reply.started":"2024-06-17T07:22:44.878067Z","shell.execute_reply":"2024-06-17T07:22:44.884184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(train_test_split)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:22:53.657685Z","iopub.execute_input":"2024-06-17T07:22:53.658078Z","iopub.status.idle":"2024-06-17T07:22:53.665542Z","shell.execute_reply.started":"2024-06-17T07:22:53.658048Z","shell.execute_reply":"2024-06-17T07:22:53.664299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(df[features], df['HeartDisease'], train_size = 0.8, random_state = RANDOM_STATE)\n\n# keep the shuffle = 'True' since dataset has not any time dependency","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:23:49.836342Z","iopub.execute_input":"2024-06-17T07:23:49.836868Z","iopub.status.idle":"2024-06-17T07:23:49.853715Z","shell.execute_reply.started":"2024-06-17T07:23:49.836832Z","shell.execute_reply":"2024-06-17T07:23:49.851825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'train samples: {len(X_train)}')\nprint(f'validation samples: {len(X_val)}')\nprint(f'target proportion: {sum(y_train)/len(y_train):.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:24:04.472218Z","iopub.execute_input":"2024-06-17T07:24:04.472727Z","iopub.status.idle":"2024-06-17T07:24:04.480619Z","shell.execute_reply.started":"2024-06-17T07:24:04.472691Z","shell.execute_reply":"2024-06-17T07:24:04.478954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_samples_split_list = [2,10, 30, 50, 100, 200, 300, 700] ## if the number is an integer, then it is the actual quantity of samples\nmax_depth_list = [1,2, 3, 4, 8, 16, 32, 64, None] # 'None' means no depth limit","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:24:54.741973Z","iopub.execute_input":"2024-06-17T07:24:54.74236Z","iopub.status.idle":"2024-06-17T07:24:54.749313Z","shell.execute_reply.started":"2024-06-17T07:24:54.74233Z","shell.execute_reply":"2024-06-17T07:24:54.747726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_list_train = []\naccuracy_list_val = []\nfor min_samples_split in min_samples_split_list:\n    # can fit the model at the same time i define it, because the fit function returns the fitted estimator\n    model = DecisionTreeClassifier(min_samples_split = min_samples_split,\n                                   random_state = RANDOM_STATE).fit(X_train,y_train) \n    predictions_train = model.predict(X_train) ## predicted values for the train dataset\n    predictions_val = model.predict(X_val) ## predicted values for the test dataset\n    accuracy_train = accuracy_score(predictions_train,y_train)\n    accuracy_val = accuracy_score(predictions_val,y_val)\n    accuracy_list_train.append(accuracy_train)\n    accuracy_list_val.append(accuracy_val)\n\nplt.title('Train x Validation metrics')\nplt.xlabel('min_samples_split')\nplt.ylabel('accuracy')\nplt.xticks(ticks = range(len(min_samples_split_list )),labels=min_samples_split_list)\nplt.plot(accuracy_list_train)\nplt.plot(accuracy_list_val)\nplt.legend(['Train','Validation'])","metadata":{"execution":{"iopub.status.busy":"2024-06-17T07:26:03.565701Z","iopub.execute_input":"2024-06-17T07:26:03.566663Z","iopub.status.idle":"2024-06-17T07:26:04.102436Z","shell.execute_reply.started":"2024-06-17T07:26:03.566611Z","shell.execute_reply":"2024-06-17T07:26:04.101195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"note how increasing the the number of `min_samples_split` reduces overfitting.\n- increasing `min_samples_split` from 10 to 30, and from 30 to 50, even though it does not improve the validation accuracy, it brings the training accuracy closer to it, showing a reduction in overfitting.\n\n--> do the same experiment with `max_depth`.","metadata":{}},{"cell_type":"code","source":"accuracy_list_train = []\naccuracy_list_val = []\nfor max_depth in max_depth_list:\n    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n    model = DecisionTreeClassifier(max_depth = max_depth,\n                                   random_state = RANDOM_STATE).fit(X_train,y_train) \n    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n    accuracy_train = accuracy_score(predictions_train,y_train)\n    accuracy_val = accuracy_score(predictions_val,y_val)\n    accuracy_list_train.append(accuracy_train)\n    accuracy_list_val.append(accuracy_val)\n\nplt.title('Train x Validation metrics')\nplt.xlabel('max_depth')\nplt.ylabel('accuracy')\nplt.xticks(ticks = range(len(max_depth_list )),labels=max_depth_list)\nplt.plot(accuracy_list_train)\nplt.plot(accuracy_list_val)\nplt.legend(['Train','Validation'])","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:30:12.383672Z","iopub.execute_input":"2024-06-17T08:30:12.384155Z","iopub.status.idle":"2024-06-17T08:30:12.826365Z","shell.execute_reply.started":"2024-06-17T08:30:12.384121Z","shell.execute_reply":"2024-06-17T08:30:12.82515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as seen in general -> reducing `max_depth` can help to reduce overfitting.\n- reducing `max_depth` from 8 to 4 increases validation accuracy closer to training accuracy, while significantly reducing training accuracy.\n- the validation accuracy reaches the highest at tree_depth=4. \n- when the `max_depth` is smaller than 3, both training and validation accuracy decreases.  The tree cannot make enough splits to distinguish positives from negatives (the model is underfitting the training set). \n- when the `max_depth` is too high ( >= 5), validation accuracy decreases while training accuracy increases, indicating that the model is overfitting to the training set.\n\nso we can choose the best values for these two hyper-parameters for our model to be:\n- `max_depth = 4`\n- `min_samples_split = 50` ","metadata":{}},{"cell_type":"code","source":"decision_tree_model = DecisionTreeClassifier(min_samples_split = 50,\n                                             max_depth = 3,\n                                             random_state = RANDOM_STATE).fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:31:18.127899Z","iopub.execute_input":"2024-06-17T08:31:18.12833Z","iopub.status.idle":"2024-06-17T08:31:18.140953Z","shell.execute_reply.started":"2024-06-17T08:31:18.128288Z","shell.execute_reply":"2024-06-17T08:31:18.139647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(decision_tree_model.predict(X_train),y_train):.4f}\")\nprint(f\"Metrics validation:\\n\\tAccuracy score: {accuracy_score(decision_tree_model.predict(X_val),y_val):.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:31:28.74897Z","iopub.execute_input":"2024-06-17T08:31:28.749481Z","iopub.status.idle":"2024-06-17T08:31:28.766125Z","shell.execute_reply.started":"2024-06-17T08:31:28.749447Z","shell.execute_reply":"2024-06-17T08:31:28.764714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"look, ma! o sign of overfitting, even though the metrics are not that good.","metadata":{}},{"cell_type":"markdown","source":"## Random Forest\n\ntry the Random Forest algorithm also, using the Scikit-learn implementation. \n- all of the hyperparameters found in the decision tree model will also exist in this algorithm, since a random forest is an ensemble of many Decision Trees.\n- one additional hyperparameter for Random Forest is called `n_estimators` which is the number of Decision Trees that make up the Random Forest. \n\nfor a Random Forest, we randomly choose a subset of the features AND randomly choose a subset of the training examples to train each individual tree.\n- if $n$ is the number of features, we will randomly select $\\sqrt{n}$ of these features to train each individual tree. \n- note that one can modify this by setting the `max_features` parameter.\n\ncan also speed up your training jobs with another parameter, `n_jobs`. \n- since the fitting of each tree is independent of each other, it is possible fit more than one tree in parallel. \n- so setting `n_jobs` higher will increase how many CPU cores it will use. Note that the numbers very close to the maximum cores of your CPU may impact on the overall performance of your PC and even lead to freezes. \n- changing this parameter does not impact on the final result but can reduce the training time.\n\nwe will run the same script again, but with another parameter, `n_estimators`, where we will choose between 10, 50, and 100. The default is 100.","metadata":{}},{"cell_type":"code","source":"min_samples_split_list = [2,10, 30, 50, 100, 200, 300, 700]  ## If the number is an integer, then it is the actual quantity of samples,\n                                             ## If it is a float, then it is the percentage of the dataset\nmax_depth_list = [2, 4, 8, 16, 32, 64, None]\nn_estimators_list = [10,50,100,500]","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:36:18.109905Z","iopub.execute_input":"2024-06-17T08:36:18.110352Z","iopub.status.idle":"2024-06-17T08:36:18.117511Z","shell.execute_reply.started":"2024-06-17T08:36:18.110313Z","shell.execute_reply":"2024-06-17T08:36:18.115964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_list_train = []\naccuracy_list_val = []\nfor min_samples_split in min_samples_split_list:\n    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n    model = RandomForestClassifier(min_samples_split = min_samples_split,\n                                   random_state = RANDOM_STATE).fit(X_train,y_train) \n    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n    accuracy_train = accuracy_score(predictions_train,y_train)\n    accuracy_val = accuracy_score(predictions_val,y_val)\n    accuracy_list_train.append(accuracy_train)\n    accuracy_list_val.append(accuracy_val)\n\nplt.title('Train x Validation metrics')\nplt.xlabel('min_samples_split')\nplt.ylabel('accuracy')\nplt.xticks(ticks = range(len(min_samples_split_list )),labels=min_samples_split_list) \nplt.plot(accuracy_list_train)\nplt.plot(accuracy_list_val)\nplt.legend(['Train','Validation'])","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:36:28.398061Z","iopub.execute_input":"2024-06-17T08:36:28.398517Z","iopub.status.idle":"2024-06-17T08:36:31.023639Z","shell.execute_reply.started":"2024-06-17T08:36:28.398471Z","shell.execute_reply":"2024-06-17T08:36:31.022348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NOTE: even though the validation accuraty reaches is the same both at `min_samples_split = 2` and `min_samples_split = 10`, in the latter the difference in training and validation set reduces, showing less overfitting.","metadata":{}},{"cell_type":"code","source":"accuracy_list_train = []\naccuracy_list_val = []\nfor max_depth in max_depth_list:\n    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n    model = RandomForestClassifier(max_depth = max_depth,\n                                   random_state = RANDOM_STATE).fit(X_train,y_train) \n    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n    accuracy_train = accuracy_score(predictions_train,y_train)\n    accuracy_val = accuracy_score(predictions_val,y_val)\n    accuracy_list_train.append(accuracy_train)\n    accuracy_list_val.append(accuracy_val)\n\nplt.title('Train x Validation metrics')\nplt.xlabel('max_depth')\nplt.ylabel('accuracy')\nplt.xticks(ticks = range(len(max_depth_list )),labels=max_depth_list)\nplt.plot(accuracy_list_train)\nplt.plot(accuracy_list_val)\nplt.legend(['Train','Validation'])","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:37:23.173469Z","iopub.execute_input":"2024-06-17T08:37:23.173911Z","iopub.status.idle":"2024-06-17T08:37:25.626672Z","shell.execute_reply.started":"2024-06-17T08:37:23.173878Z","shell.execute_reply":"2024-06-17T08:37:25.625507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_list_train = []\naccuracy_list_val = []\nfor n_estimators in n_estimators_list:\n    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n    model = RandomForestClassifier(n_estimators = n_estimators,\n                                   random_state = RANDOM_STATE).fit(X_train,y_train) \n    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n    accuracy_train = accuracy_score(predictions_train,y_train)\n    accuracy_val = accuracy_score(predictions_val,y_val)\n    accuracy_list_train.append(accuracy_train)\n    accuracy_list_val.append(accuracy_val)\n\nplt.title('Train x Validation metrics')\nplt.xlabel('n_estimators')\nplt.ylabel('accuracy')\nplt.xticks(ticks = range(len(n_estimators_list )),labels=n_estimators_list)\nplt.plot(accuracy_list_train)\nplt.plot(accuracy_list_val)\nplt.legend(['Train','Validation'])","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:38:09.358457Z","iopub.execute_input":"2024-06-17T08:38:09.358975Z","iopub.status.idle":"2024-06-17T08:38:11.955995Z","shell.execute_reply.started":"2024-06-17T08:38:09.358938Z","shell.execute_reply":"2024-06-17T08:38:11.954566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now fit a random forest with the following parameters:\n\n - max_depth: 16\n - min_samples_split: 10\n - n_estimators: 100","metadata":{}},{"cell_type":"code","source":"random_forest_model = RandomForestClassifier(n_estimators = 100,\n                                             max_depth = 16, \n                                             min_samples_split = 10).fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:39:15.101789Z","iopub.execute_input":"2024-06-17T08:39:15.102745Z","iopub.status.idle":"2024-06-17T08:39:15.393234Z","shell.execute_reply.started":"2024-06-17T08:39:15.102703Z","shell.execute_reply":"2024-06-17T08:39:15.392069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(random_forest_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(random_forest_model.predict(X_val),y_val):.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:39:23.303912Z","iopub.execute_input":"2024-06-17T08:39:23.304386Z","iopub.status.idle":"2024-06-17T08:39:23.346645Z","shell.execute_reply.started":"2024-06-17T08:39:23.304351Z","shell.execute_reply":"2024-06-17T08:39:23.345455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NOTE: we are searching for the best value one hyperparameter while leaving the other hyperparameters at their default values.\n- ideally, we would want to check every combination of values for every hyperparameter that we are tuning.\n- if we have 3 hyperparameters, and each hyperparameter has 4 values to try out, we should have a total of 4 x 4 x 4 = 64 combinations to try.\n- When we only modify one hyperparameter while leaving the rest as their default value, we are trying 4 + 4 + 4 = 12 results. \n- to try out all combinations, we can use a sklearn implementation called GridSearchCV. GridSearchCV has a refit parameter that will automatically refit a model on the best combination so we will not need to program it explicitly. **scikit** reference: [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).","metadata":{}},{"cell_type":"markdown","source":"## 4.3 XGBoost\n\nNEXT UP! the Gradient Boosting model, called XGBoost. the boosting methods train several trees, but instead of them being uncorrelated to each other, now the trees are fit one after the other in order to minimize the error. \n\nmodel has the same parameters as a decision tree, plus the learning rate.\n- the learning rate is the size of the step on the Gradient Descent method that the XGBoost uses internally to minimize the error on each train step.\n\none interesting thing about the XGBoost is that during fitting, it can take in an evaluation dataset of the form `(X_val,y_val)`.\n- on each iteration, it measures the cost (or evaluation metric) on the evaluation datasets.\n- once the cost (or metric) stops decreasing for a number of rounds (called early_stopping_rounds), the training will stop. \n- more iterations lead to more estimators, and more estimators can result in overfitting.  \n- by stopping once the validation metric no longer improves, we can limit the number of estimators created, and reduce overfitting.\n\nfirst, define a subset of our training set (we should not use the test set here).","metadata":{}},{"cell_type":"code","source":"n = int(len(X_train)*0.8) ## use 80% to train and 20% to eval","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:41:47.769854Z","iopub.execute_input":"2024-06-17T08:41:47.770283Z","iopub.status.idle":"2024-06-17T08:41:47.776289Z","shell.execute_reply.started":"2024-06-17T08:41:47.770251Z","shell.execute_reply":"2024-06-17T08:41:47.774922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_fit, X_train_eval, y_train_fit, y_train_eval = X_train[:n], X_train[n:], y_train[:n], y_train[n:]","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:41:54.095022Z","iopub.execute_input":"2024-06-17T08:41:54.095418Z","iopub.status.idle":"2024-06-17T08:41:54.102807Z","shell.execute_reply.started":"2024-06-17T08:41:54.095387Z","shell.execute_reply":"2024-06-17T08:41:54.101286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)\nxgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)], early_stopping_rounds = 10)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:42:03.843264Z","iopub.execute_input":"2024-06-17T08:42:03.843781Z","iopub.status.idle":"2024-06-17T08:42:04.085414Z","shell.execute_reply.started":"2024-06-17T08:42:03.843743Z","shell.execute_reply":"2024-06-17T08:42:04.084352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model.best_iteration","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:42:24.959834Z","iopub.execute_input":"2024-06-17T08:42:24.960288Z","iopub.status.idle":"2024-06-17T08:42:24.969275Z","shell.execute_reply.started":"2024-06-17T08:42:24.960253Z","shell.execute_reply":"2024-06-17T08:42:24.967896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_val),y_val):.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:42:34.889337Z","iopub.execute_input":"2024-06-17T08:42:34.890254Z","iopub.status.idle":"2024-06-17T08:42:34.918804Z","shell.execute_reply.started":"2024-06-17T08:42:34.890214Z","shell.execute_reply":"2024-06-17T08:42:34.917681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"in this example, both Random Forest and XGBoost had similar performance (test accuracy).","metadata":{}}]}